{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing with pandas\n",
    "In this notebook, I am creating a pandas dataframe from the raw JSON data obtained from scrapper, which will subsequently be uploaded to an Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['uuids', 'website_ids', 'gene_function', 'spatial_expression_patterns', 'cellular_expression_patterns', 'begining', 'termination', 'detailed_expression_patterns', 'promoters', 'strain_information', 'strain_name', 'date_created', 'source', 'reporter', 'lineage', 'construct', 'created_by', 'construct_info', 'plasmid_name', 'gene', 'transcript', 'promoter_length', 'left', 'forward', 'right', 'reverse', 'vector', 'expressing_strains', 'image_urls'])\n"
     ]
    }
   ],
   "source": [
    "# load raw data\n",
    "with open(\"raw_data/data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uuids', 75), ('website_ids', 75), ('gene_function', 75), ('spatial_expression_patterns', 75), ('cellular_expression_patterns', 75), ('begining', 75), ('termination', 75), ('detailed_expression_patterns', 75), ('promoters', 75), ('strain_information', 75), ('strain_name', 75), ('date_created', 75), ('source', 75), ('reporter', 75), ('lineage', 75), ('construct', 75), ('created_by', 75), ('construct_info', 75), ('plasmid_name', 75), ('gene', 75), ('transcript', 75), ('promoter_length', 75), ('left', 75), ('forward', 75), ('right', 75), ('reverse', 75), ('vector', 75), ('expressing_strains', 75), ('image_urls', 74)]\n"
     ]
    }
   ],
   "source": [
    "# check if all values of each key are of the same length\n",
    "print(list(zip(data.keys(), [len(value) for value in data.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image urls are 1 short from all the other values. We need to find the uuid corresponding to the missing url. We can compare th website_ids and image_urls to find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103', '24', '104', '105', '107', '20', '106', '108', '136', '109', '110', '111', '113', '135', '114', '112', '128', '118', '119', '120', '131', '133', '132', '130', '129', '61', '115', '121', '64', '134', '127', '126', '124', '125', '70', '123', '122', '78', '79', '81', '82', '84', '85', '101', '90', '91', '93', '116', '102', '98', '100', '117', '137', '140', '141', '143', '144', '145', '146', '152', '149', '151', '156', '159', '157', '158', '160', '175', '176', '177', '178', '179', '182', '180']\n"
     ]
    }
   ],
   "source": [
    "# parse image_urls to extract website_ids\n",
    "ids = [link.split(\"id=\")[1] for link in data[\"image_urls\"]]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '140',\n",
       " '141',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '149',\n",
       " '151',\n",
       " '152',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '180',\n",
       " '182',\n",
       " '84',\n",
       " '85',\n",
       " '90',\n",
       " '91',\n",
       " '93',\n",
       " '98'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ids).difference(set(list(data[\"website_ids\"])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033258918218bad17b8b749ba72d9f5ce500477fc2a8df2be89d008491fa6be0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
